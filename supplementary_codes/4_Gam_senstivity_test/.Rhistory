colour="black"), size = 0.3) +
xlab("Year (CE)") +
ylab("RSL (m)") +
theme_bw()+
labs(colour = "")+
theme(strip.text.x = element_text(size = 5),
strip.background =element_rect(fill=c("white")))+
scale_fill_manual('',
values = 'grey',
labels = expression(paste("1",sigma," " ,"RSL and age uncertainty")),
guide = guide_legend(override.aes = list(alpha = 0.7))) +
scale_colour_manual(values = c("black"),
labels=c("Proxy Records"))+
theme(legend.position=c(1,0),legend.justification = c(1,0), legend.box = "horizontal")+
guides(color = guide_legend(override.aes = list(size = 3)))+
theme(plot.title = element_text(size=18,face="bold"),
axis.title=element_text(size=12,face="bold"),
legend.text=element_text(size=10))+
facet_wrap(~SiteName)
plot_TG_data_100
ggsave(plot_TG_data_100, file = "fig/TG_plot_1_over_100yrs.pdf", width = 10, height = 6)
SL_df$data_type_id %>% unique()
# Range of Data as new column----
SL_df <- SL_df %>%
group_by(SiteName) %>%
mutate(Age_range = max(Age*1000) - min(Age*1000))
subset(SL_df,data_type_id =="TideGaugeData" & Age_range >= 100)
####----Noisy Input GAM for RSL along Atlantic coast of North America---------
# Data: NA Atlantic coast = Proxies & tide gauges(1 deg from proxy & greater than 20 observations)
# Process Level: f(x,t) = r(t) + g(z_x) + h(z_x) + l(x,t) + epilson
#             r(t) = r = Regional component(B-spline in time)
#             g(z_x) = g_z_x = Linear Local component(Random Effect)
#             h(z_x) = h_z_x = Site Specific vertical offset(Random Effect)
#             l(x,t) = l = Non-Linear Local component(B-spline in space time)
# Priors: Linear local component informed by linear regression through data prior to 1800 & for TG rate from ICE5G with uncertainty from Engelhart 2011
# Model Fitting:1. Model run first time without Non-linear local component. Estimate for b_r & sigma_h is obtained.
#               2. Noisy Input corrective variance term estimated.
#               3. Model re-run with non-linear local component, corrective variance term included and priors for Regional component & site specific vertical offset are informed
# Clear workspace
rm(list = ls())
#---------Set working directory--------------
setwd('/Users/mupton/Library/CloudStorage/Dropbox/PhD_Work_2019_2023/JRSS_seriesC_2022/0.Revisions_2024/codes')
#setwd('/users/research/mupton/1. NIGAM_paper_2022/0. NIGAM')
#----------Load packages--------------------
#library(tidyverse)
library(dplyr)
library(readr)
library(tidyr)
library(geosphere) #distm
library(R2jags)
library(splines)
library(dbarts)# dummy matrix creation
library(ncdf4) # package for netcdf manipulation
#library(rnaturalearth)#n_states for map
library("ggrepel") #label_repel on map
#library("ggspatial")#annotation_scale in map
library(ggplot2)
library(ggtext)
#library(shinystan)
#-----Functions-----
source("R/clean_SL_data.R")
source("R/clean_tidal_gauge_data.R")
source("R/linear_reg_rates.R")
source("R/match.closest.R")
source("R/add_GIA_rate.R")
source("R/plot_data.R")
source("R/bs_bbase.R")
source("R/spline_basis_function.R")
source("R/run_spline.R")
source("R/add_noise.R")
source("R/run_noise_spline.R")
source("R/plot_results.R")
source("R/rate_of_change_fun.R")
source("R/plot_4_sites_results.R")
source("R/dataframe_mod_outputs.R")
#---Read in SL data from  proxy records---
SL_data <- readr::read_csv("https://raw.githubusercontent.com/maeveupton/NI-GAM/main/data/Common%20Era%20Database%202022_a.csv")
new_col_names <- c('Basin', 'Region','Site','Reference','Indicator','Latitude','Longitude',
'RSL','RSL_er_max','RSL_er_min','Age','Age_2_er_max','Age_2_er_min')
names(SL_data) <- new_col_names
#---Cleaning SL data----
SL_df_raw <- clean_SL_data(SL_df=SL_data,SL_data=SL_data,
save_loc="data/SL_df_clean.csv")
#--Clean tidal gauge data from PSMSL---
SL_df <- clean_tidal_gauge_data(SL_df=SL_df_raw,
save_loc="data/SL_tidal_gauge_df_new.csv")
# Using data from PSMSL website for annual tide gauge data----------------------------------
# Set up the URL for downloading the data
url <- "https://psmsl.org/data/obtaining/rlr.annual.data/rlr_annual.zip"
# Create a temporary file
temp_file <- tempfile()
# Download the file and save it to the temporary file
utils::download.file(url,
destfile = temp_file,
quiet = TRUE)
# Unzip the data file to a temporary directory
temp_dir <- tempfile()
utils::unzip(temp_file, exdir = temp_dir)
### ------------Loop to open all RSL & Age data files------------
read_plus <- function(flnm) {
# fread quicker way to read in & allows for ; to be used
data.table::fread(flnm, sep = ";") %>%
# allows you to include the file name as id
dplyr::mutate(filename = flnm)
}
# Warnings: there are some files without data
suppressWarnings(
temp_SL <-
list.files(
path = file.path(temp_dir, "rlr_annual", "data"),
pattern = "*.rlrdata",
full.names = T
) %>%
purrr::map_df(~ read_plus(.)) %>%
dplyr::tibble()
)
colnames(temp_SL) <- c("Age", "RSL", "flag_attention_1", "flag_attention_2", "id")
temp_SL$id <- stringr::str_extract(basename(temp_SL$id), "[0-9]+")
# Access the individual data files within the 'rlr_annual' folder
file_path <- file.path(temp_dir, "rlr_annual", "filelist.txt")
file_list <- utils::read.csv(file_path, stringsAsFactors = FALSE, header = F, sep = ";")
colnames(file_list) <- c(
"id", "Latitude", "Longitude", "name",
"coastline", "stationcode", "stationflag"
)
# Removing white space in the name of each site
file_list$name <- stringr::str_trim(file_list$name, side = "both")
file_list$stationflag <- stringr::str_trim(file_list$stationflag, side = "both")
# Data from the PSMSL website
data_TG <- temp_SL %>%
# Pulling out the file number from string so that it matches the name from other files
dplyr::mutate(id = stringr::str_extract(basename(temp_SL$id), "[0-9]+")) %>%
# Cases where bad data was collected
dplyr::filter(!RSL == -99999) # %>%
# Updating the RSL to the shifted RSL value using PSMSL instructions
data_TG$RSL <- data_TG$RSL - 7000 # data_TG$RSL_offset #
#--Joining SL data with location names--
annual_SL_tide_df <- base::merge(data_TG, file_list, by = "id", all = TRUE)
#-- Removing sites which have a station flag raised as they are poor sites---
annual_SL_tide_df <- annual_SL_tide_df %>%
dplyr::filter(!stationflag == "Y") %>%
tidyr::drop_na()
# Remove the temporary file and directory
unlink(temp_file)
unlink(temp_dir, recursive = TRUE)
# Annual Tidal Gauge data----
annual_tidal_gauge_data_df <- annual_SL_tide_df %>%
dplyr::select(
Age, RSL, Latitude, name,
# RSL_offset, Age_epoch_id,
Longitude
) %>%
dplyr::rename(SiteName = name) %>%
# from mm --> m
dplyr::mutate(RSL = RSL / 1000) %>%
# Reordering by group
dplyr::group_by(SiteName) %>%
dplyr::arrange(SiteName, .by_group = TRUE) %>%
dplyr::arrange(Age) %>%
dplyr::mutate(data_type_id = "TideGaugeData")
# Set the window size for the moving average (in this case, 10 years)
# Rate of sedimentation for proxies when using continuous cores
window_size <- sediment_average_TG
sediment_average_TG = 10
# Set the window size for the moving average (in this case, 10 years)
# Rate of sedimentation for proxies when using continuous cores
window_size <- sediment_average_TG
# Version B: Decadal Averages using simple method------
decadal_averages_TG <-
annual_tidal_gauge_data_df %>%
dplyr::mutate(decade = (Age - 1) %/% window_size) %>%
dplyr::group_by(decade, SiteName) %>%
dplyr::summarise(
# decade_meanRSL = mean(RSL)#,
rolling_avg = mean(RSL),
Age = max(Age) # ,
# rows_site = dplyr::n()
) # Age=min(Age)
# Using standard deviation of RSL over the decade as uncertainty----
decadal_averages_TG <- decadal_averages_TG %>%
dplyr::group_by(SiteName) %>%
# dplyr::mutate(sd_TG = sd(decade_meanRSL))
dplyr::mutate(sd_TG = sd(rolling_avg))
#----- New df with decadal averages for tide gauges-----
tidal_gauge_average_10_df <- base::merge(
decadal_averages_TG,
annual_tidal_gauge_data_df
)
#---Rsl & Age error for tidal gauge data----
tidal_gauge_full_df <- tidal_gauge_average_10_df %>%
dplyr::mutate(
Age_err = 2.5, # years --> half a year/half a decade, 2.5 years either side.
) %>%
# dplyr::mutate(sd_TG = ifelse(is.na(sd_TG), 0.001, sd_TG)) %>%
dplyr::group_by(SiteName) %>%
dplyr::mutate(
RSL_err = 0.001 # sd_TG
# RSL_err = sd_TG
)
tidal_gauge_full_df <- tidal_gauge_full_df %>%
dplyr::mutate(Age = Age / 1000) %>%
dplyr::mutate(Age_err = Age_err / 1000) %>%
dplyr::mutate(RSL_annual = RSL) %>%
# Change for Version A
dplyr::mutate(RSL = rolling_avg) %>%
# Change for Version B
# dplyr::mutate(RSL = decade_meanRSL) %>%
dplyr::select(!c(
decade, # Age_epoch_id,RSL_offset,
rolling_avg,
RSL_annual
))
# No user option here -> this is a must: Removing sites with only 2 points (20 years of data)-----
decadal_TG_df <-
tidal_gauge_full_df %>%
dplyr::group_by(SiteName) %>%
dplyr::filter(dplyr::n() > 2)
#-----Uniting original dataset and model run to give a site index to model_result data set-----
SL_site_df<- data %>%
mutate(Longitude = round(Longitude,1)) %>%
mutate(Latitude = round(Latitude,1)) %>%
unite("LongLat",Latitude:Longitude,remove = FALSE) %>%  #Uniting 2 columns
mutate(site = sprintf("%02d", as.integer(as.factor(LongLat)))) %>%
mutate(data_type_id = "ProxyData") %>%
group_by(SiteName) %>%
mutate(Longitude = first(Longitude),
Latitude = first(Latitude))
data
#-----Uniting original dataset and model run to give a site index to model_result data set-----
SL_site_df<- SL_df %>%
mutate(Longitude = round(Longitude,1)) %>%
mutate(Latitude = round(Latitude,1)) %>%
unite("LongLat",Latitude:Longitude,remove = FALSE) %>%  #Uniting 2 columns
mutate(site = sprintf("%02d", as.integer(as.factor(LongLat)))) %>%
mutate(data_type_id = "ProxyData") %>%
group_by(SiteName) %>%
mutate(Longitude = first(Longitude),
Latitude = first(Latitude))
####----Noisy Input GAM for RSL along Atlantic coast of North America---------
# Data: NA Atlantic coast = Proxies & tide gauges(1 deg from proxy & greater than 20 observations)
# Process Level: f(x,t) = r(t) + g(z_x) + h(z_x) + l(x,t) + epilson
#             r(t) = r = Regional component(B-spline in time)
#             g(z_x) = g_z_x = Linear Local component(Random Effect)
#             h(z_x) = h_z_x = Site Specific vertical offset(Random Effect)
#             l(x,t) = l = Non-Linear Local component(B-spline in space time)
# Priors: Linear local component informed by linear regression through data prior to 1800 & for TG rate from ICE5G with uncertainty from Engelhart 2011
# Model Fitting:1. Model run first time without Non-linear local component. Estimate for b_r & sigma_h is obtained.
#               2. Noisy Input corrective variance term estimated.
#               3. Model re-run with non-linear local component, corrective variance term included and priors for Regional component & site specific vertical offset are informed
# Clear workspace
rm(list = ls())
#---------Set working directory--------------
setwd('/Users/mupton/Library/CloudStorage/Dropbox/PhD_Work_2019_2023/JRSS_seriesC_2022/0.Revisions_2024/codes')
#setwd('/users/research/mupton/1. NIGAM_paper_2022/0. NIGAM')
#----------Load packages--------------------
#library(tidyverse)
library(dplyr)
library(readr)
library(tidyr)
library(geosphere) #distm
library(R2jags)
library(splines)
library(dbarts)# dummy matrix creation
library(ncdf4) # package for netcdf manipulation
#library(rnaturalearth)#n_states for map
library("ggrepel") #label_repel on map
#library("ggspatial")#annotation_scale in map
library(ggplot2)
library(ggtext)
#library(shinystan)
#-----Functions-----
source("R/clean_SL_data.R")
source("R/clean_tidal_gauge_data.R")
source("R/linear_reg_rates.R")
source("R/match.closest.R")
source("R/add_GIA_rate.R")
source("R/plot_data.R")
source("R/bs_bbase.R")
source("R/spline_basis_function.R")
source("R/run_spline.R")
source("R/add_noise.R")
source("R/run_noise_spline.R")
source("R/plot_results.R")
source("R/rate_of_change_fun.R")
source("R/plot_4_sites_results.R")
source("R/dataframe_mod_outputs.R")
#---Read in SL data from  proxy records---
SL_data <- readr::read_csv("https://raw.githubusercontent.com/maeveupton/NI-GAM/main/data/Common%20Era%20Database%202022_a.csv")
new_col_names <- c('Basin', 'Region','Site','Reference','Indicator','Latitude','Longitude',
'RSL','RSL_er_max','RSL_er_min','Age','Age_2_er_max','Age_2_er_min')
names(SL_data) <- new_col_names
#---Cleaning SL data----
SL_df_raw <- clean_SL_data(SL_df=SL_data,SL_data=SL_data,
save_loc="data/SL_df_clean.csv")
#--Clean tidal gauge data from PSMSL---
SL_df <- clean_tidal_gauge_data(SL_df=SL_df_raw,
save_loc="data/SL_tidal_gauge_df_new.csv")
# Using data from PSMSL website for annual tide gauge data----------------------------------
# Set up the URL for downloading the data
url <- "https://psmsl.org/data/obtaining/rlr.annual.data/rlr_annual.zip"
# Create a temporary file
temp_file <- tempfile()
# Download the file and save it to the temporary file
utils::download.file(url,
destfile = temp_file,
quiet = TRUE)
# Unzip the data file to a temporary directory
temp_dir <- tempfile()
utils::unzip(temp_file, exdir = temp_dir)
### ------------Loop to open all RSL & Age data files------------
read_plus <- function(flnm) {
# fread quicker way to read in & allows for ; to be used
data.table::fread(flnm, sep = ";") %>%
# allows you to include the file name as id
dplyr::mutate(filename = flnm)
}
# Warnings: there are some files without data
suppressWarnings(
temp_SL <-
list.files(
path = file.path(temp_dir, "rlr_annual", "data"),
pattern = "*.rlrdata",
full.names = T
) %>%
purrr::map_df(~ read_plus(.)) %>%
dplyr::tibble()
)
colnames(temp_SL) <- c("Age", "RSL", "flag_attention_1", "flag_attention_2", "id")
temp_SL$id <- stringr::str_extract(basename(temp_SL$id), "[0-9]+")
# Access the individual data files within the 'rlr_annual' folder
file_path <- file.path(temp_dir, "rlr_annual", "filelist.txt")
file_list <- utils::read.csv(file_path, stringsAsFactors = FALSE, header = F, sep = ";")
colnames(file_list) <- c(
"id", "Latitude", "Longitude", "name",
"coastline", "stationcode", "stationflag"
)
# Removing white space in the name of each site
file_list$name <- stringr::str_trim(file_list$name, side = "both")
file_list$stationflag <- stringr::str_trim(file_list$stationflag, side = "both")
# Data from the PSMSL website
data_TG <- temp_SL %>%
# Pulling out the file number from string so that it matches the name from other files
dplyr::mutate(id = stringr::str_extract(basename(temp_SL$id), "[0-9]+")) %>%
# Cases where bad data was collected
dplyr::filter(!RSL == -99999) # %>%
# Updating the RSL to the shifted RSL value using PSMSL instructions
data_TG$RSL <- data_TG$RSL - 7000 # data_TG$RSL_offset #
#--Joining SL data with location names--
annual_SL_tide_df <- base::merge(data_TG, file_list, by = "id", all = TRUE)
#-- Removing sites which have a station flag raised as they are poor sites---
annual_SL_tide_df <- annual_SL_tide_df %>%
dplyr::filter(!stationflag == "Y") %>%
tidyr::drop_na()
# Remove the temporary file and directory
unlink(temp_file)
unlink(temp_dir, recursive = TRUE)
# Annual Tidal Gauge data----
annual_tidal_gauge_data_df <- annual_SL_tide_df %>%
dplyr::select(
Age, RSL, Latitude, name,
# RSL_offset, Age_epoch_id,
Longitude
) %>%
dplyr::rename(SiteName = name) %>%
# from mm --> m
dplyr::mutate(RSL = RSL / 1000) %>%
# Reordering by group
dplyr::group_by(SiteName) %>%
dplyr::arrange(SiteName, .by_group = TRUE) %>%
dplyr::arrange(Age) %>%
dplyr::mutate(data_type_id = "TideGaugeData")
# Set the window size for the moving average (in this case, 10 years)
# Rate of sedimentation for proxies when using continuous cores
window_size <- sediment_average_TG
sediment_average_TG = 10
# Set the window size for the moving average (in this case, 10 years)
# Rate of sedimentation for proxies when using continuous cores
window_size <- sediment_average_TG
# Version B: Decadal Averages using simple method------
decadal_averages_TG <-
annual_tidal_gauge_data_df %>%
dplyr::mutate(decade = (Age - 1) %/% window_size) %>%
dplyr::group_by(decade, SiteName) %>%
dplyr::summarise(
# decade_meanRSL = mean(RSL)#,
rolling_avg = mean(RSL),
Age = max(Age) # ,
# rows_site = dplyr::n()
) # Age=min(Age)
# Using standard deviation of RSL over the decade as uncertainty----
decadal_averages_TG <- decadal_averages_TG %>%
dplyr::group_by(SiteName) %>%
# dplyr::mutate(sd_TG = sd(decade_meanRSL))
dplyr::mutate(sd_TG = sd(rolling_avg))
#----- New df with decadal averages for tide gauges-----
tidal_gauge_average_10_df <- base::merge(
decadal_averages_TG,
annual_tidal_gauge_data_df
)
#---Rsl & Age error for tidal gauge data----
tidal_gauge_full_df <- tidal_gauge_average_10_df %>%
dplyr::mutate(
Age_err = 2.5, # years --> half a year/half a decade, 2.5 years either side.
) %>%
# dplyr::mutate(sd_TG = ifelse(is.na(sd_TG), 0.001, sd_TG)) %>%
dplyr::group_by(SiteName) %>%
dplyr::mutate(
RSL_err = 0.001 # sd_TG
# RSL_err = sd_TG
)
tidal_gauge_full_df <- tidal_gauge_full_df %>%
dplyr::mutate(Age = Age / 1000) %>%
dplyr::mutate(Age_err = Age_err / 1000) %>%
dplyr::mutate(RSL_annual = RSL) %>%
# Change for Version A
dplyr::mutate(RSL = rolling_avg) %>%
# Change for Version B
# dplyr::mutate(RSL = decade_meanRSL) %>%
dplyr::select(!c(
decade, # Age_epoch_id,RSL_offset,
rolling_avg,
RSL_annual
))
# No user option here -> this is a must: Removing sites with only 2 points (20 years of data)-----
decadal_TG_df <-
tidal_gauge_full_df %>%
dplyr::group_by(SiteName) %>%
dplyr::filter(dplyr::n() > 2)
#-----Uniting original dataset and model run to give a site index to model_result data set-----
SL_site_df<- SL_df %>%
mutate(Longitude = round(Longitude,1)) %>%
mutate(Latitude = round(Latitude,1)) %>%
unite("LongLat",Latitude:Longitude,remove = FALSE) %>%  #Uniting 2 columns
mutate(site = sprintf("%02d", as.integer(as.factor(LongLat)))) %>%
mutate(data_type_id = "ProxyData") %>%
group_by(SiteName) %>%
mutate(Longitude = first(Longitude),
Latitude = first(Latitude))
SL_df=SL_df_raw
#-----Uniting original dataset and model run to give a site index to model_result data set-----
SL_site_df<- SL_df %>%
mutate(Longitude = round(Longitude,1)) %>%
mutate(Latitude = round(Latitude,1)) %>%
unite("LongLat",Latitude:Longitude,remove = FALSE) %>%  #Uniting 2 columns
mutate(site = sprintf("%02d", as.integer(as.factor(LongLat)))) %>%
mutate(data_type_id = "ProxyData") %>%
group_by(SiteName) %>%
mutate(Longitude = first(Longitude),
Latitude = first(Latitude))
SL_tide_site_df<- decadal_TG_df %>%
#dplyr::select(!all_tidal_data_sites) %>%
mutate(Longitude = round(Longitude,1)) %>%
mutate(Latitude = round(Latitude,1)) %>%
unite("LongLat",Latitude:Longitude,remove=FALSE) %>%  #Uniting 2 columns
mutate(site = sprintf("%02d", as.integer(as.factor(LongLat)))) %>%
mutate(data_type_id = "TideGaugeData")
#------Joining proxy dataframe to Tide gauges data----
SL_tide_proxy <- bind_rows(SL_site_df,SL_tide_site_df)
#------Joining proxy sites to gauges based on shortest distance----
SL_proxy_unique <- SL_site_df %>%
dplyr::select(SiteName, Longitude, Latitude,data_type_id) %>%
unique()
SL_tide_unique <-  SL_tide_site_df %>%
dplyr::select(SiteName, Longitude, Latitude,data_type_id) %>%
unique()
#---Distance Matrix for each site to each other---
mat.distance<- distm(SL_proxy_unique[,2:3],SL_tide_unique[,2:3],
fun = distGeo)
mat.distance <-  as.matrix(mat.distance)
#--finding row mins & corresponding tidal gauge--
rownames(mat.distance) = SL_proxy_unique$SiteName
colnames(mat.distance) = SL_tide_unique$SiteName
#--finding row mins & corresponding tidal gauge--
dist_TG_proxy <- t(sapply(seq(nrow(mat.distance)), function(z) {
js <- order(mat.distance[z,])[1:4]
c(rownames(mat.distance)[z], colnames(mat.distance)[js[1]], mat.distance[z,js[1]],
colnames(mat.distance)[js[2]], mat.distance[z,js[2]],
colnames(mat.distance)[js[3]], mat.distance[z,js[3]],
colnames(mat.distance)[js[4]], mat.distance[z,js[4]])
}))
dist_TG_proxy <- as.data.frame(dist_TG_proxy)
colnames(dist_TG_proxy) <- c("nearest_proxy_site", "SiteName1", "min_dist1",
"SiteName_duplicate2", "min_dist2","SiteName_duplicate3",
"min_dist3","SiteName_duplicate4","min_dist4")
# Sorting the minimum distances from lowest to highest
dist_TG_proxy <- dist_TG_proxy %>% arrange(desc(min_dist1))
# TG near the proxy sites & TG longer than 150 years (New York(The Battery))
all_nearest_TG <- dist_TG_proxy %>%
dplyr::select(!c(nearest_proxy_site)) %>%
pivot_longer(cols = starts_with("SiteName"),
values_to = "SiteName") %>%
dplyr::select(!name) %>%
pivot_longer(cols = starts_with("min_dist"),
values_to = "MinimumDistance")
# 1 degree away from proxy is 111.1km
all_nearest_TG_closest <- all_nearest_TG %>% filter(MinimumDistance>111100)
# Joining the selected TG sites back with the original data
join_new_index_tide_df <- SL_tide_site_df %>%
filter(SiteName %in% all_nearest_TG_closest$SiteName)
#--There will be NAs were the proxy data doesn't have a corresponding index--
SL_df_tide_proxy <- plyr::rbind.fill(SL_site_df,
join_new_index_tide_df)#stacking rows
# Ensuring the SiteName is a factor
SL_df <- SL_df_tide_proxy %>%
dplyr::select(!c(RSL_annual,Age_epoch_id,
RSL_offset,sd_TG,rows_site,decade_meanRSL,
Indicator,Basin,Age_2_er_max,Age_2_er_min))%>%
mutate(SiteName = as.factor(SiteName))
SL_df_tide_proxy
glimpse(SL_df_tide_proxy)
source("~/Library/CloudStorage/Dropbox/PhD_Work_2019_2023/JRSS_seriesC_2022/0.Revisions_2024/codes/main.R", echo=TRUE)
source("~/Library/CloudStorage/Dropbox/PhD_Work_2019_2023/JRSS_seriesC_2022/0.Revisions_2024/codes/main.R", echo=TRUE)
